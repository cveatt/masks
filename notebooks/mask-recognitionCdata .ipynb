{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pycocotools --quiet\n!git clone https://github.com/pytorch/vision.git\n!git checkout v0.3.0\n\n!cp vision/references/detection/utils.py ./\n!cp vision/references/detection/transforms.py ./\n!cp vision/references/detection/coco_eval.py ./\n!cp vision/references/detection/engine.py ./\n!cp vision/references/detection/coco_utils.py ./","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nimport os\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom matplotlib import patches\nimport random\nfrom sklearn.model_selection import train_test_split\n\nimport cv2\nimport torch\nimport torchvision\nfrom torchvision import datasets, models, transforms\n\nimport torch.nn as nn\n\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom torch.utils import data as torch_data\nfrom torchvision import transforms as T\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\n\nfrom engine import evaluate\n\nimport time\nfrom xml.etree import ElementTree as ET","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_X_y(images_dir, annotations_dir):\n    images = sorted(os.listdir(images_dir))\n    annotations = sorted(os.listdir(annotations_dir))\n    \n    X = [os.path.join(images_dir, image) for image in images]\n    y = [os.path.join(annotations_dir, annotation) for annotation in annotations]\n    \n    return X, y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images_dir = '/kaggle/input/office-masks/Images/Images/'\nannotations_dir = '/kaggle/input/office-masks/Annotations/Annotations/'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create X and y arrays\nX, y = create_X_y(images_dir, annotations_dir)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MaskDataset(torch.utils.data.Dataset):\n    def __init__(self, images, annotations, X, y, width, height, T=None):\n        self.T = T\n        self.images = images\n        self.annotations = annotations\n        self.width = width\n        self.height = height\n        \n        self.imgs = X\n        self.annotate = y\n        #self.imgs = [image for image in sorted(os.listdir(images))]\n        #self.annotate = [image for image in sorted(os.listdir(annotations))]\n        \n        self.classes = [_, 'with_mask', 'without_mask', 'mask_weared_incorrect']\n        \n    \n    def __len__(self):\n        return len(self.imgs)\n    \n    def __getitem__(self, index):\n        image_name = self.imgs[index]\n        image_path = os.path.join(self.images, image_name)\n        \n        # Reading and converting images\n        img = cv2.imread(image_path)\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)\n        img_size = cv2.resize(img_rgb, (self.width, self.height), cv2.INTER_AREA)\n        \n        img_size /= 255.0\n        \n        # Annotation file\n        annotation_filename = self.annotate[index]\n        annotation_path = os.path.join(self.annotations, annotation_filename)\n        \n        boxes = []\n        labels =[]\n        tree = ET.parse(annotation_path)\n        root = tree.getroot()\n        \n        # Open cv file as width and height\n        wt = img.shape[1]\n        ht = img.shape[0]\n        \n        for member in root.findall('object'):\n            labels.append(self.classes.index(member.find('name').text))\n            \n            xmin = int(float(member.find('bndbox').find('xmin').text))\n            xmax = int(float(member.find('bndbox').find('xmax').text))\n            ymin = int(float(member.find('bndbox').find('ymin').text))\n            ymax = int(float(member.find('bndbox').find('ymax').text))\n            \n            # Corrected box coordinates for image size\n            xmin_cor = np.clip((xmin / wt) * self.width, 0, self.width)\n            xmax_cor = np.clip((xmax / wt) * self.width, 0, self.width)\n            ymin_cor = np.clip((ymin / ht) * self.height, 0, self.height)\n            ymax_cor = np.clip((ymax / ht) * self.height, 0, self.height)\n            \n            boxes.append([xmin_cor, ymin_cor, xmax_cor, ymax_cor])\n            \n        # Convert into a torch.Tensor\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        \n        area = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n        \n        # For not a crowd\n        iscrowd = torch.zeros((boxes.shape[0], ), dtype=torch.long)\n        \n        labels = torch.as_tensor(labels, dtype=torch.long)\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels\n        target['area'] = area\n        target['iscrowd'] = iscrowd\n        \n        image_id = torch.tensor([index])\n        target['image_id'] = image_id\n        \n        if self.T:\n            \n            sample = self.T(\n                image = img_size,\n                bboxes = target['boxes'],\n                labels = labels\n            )\n            \n            img_size = sample['image']\n            target['boxes'] = torch.Tensor(sample['bboxes'])\n            \n        return img_size, target\n    \n# Check dataset\ndataset = MaskDataset(images_dir, annotations_dir, X, y, 450, 350)\nprint('Lenght of dataset:', len(dataset), '\\n')\n\n# Getting the image and target for a test index\nimg, target = dataset[5]\nprint('Image shape:', img.shape, '\\n', 'Target:', target)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize a dictionary to store the counts of each class\nclass_counts = {label: 0 for label in dataset.classes}\n\n# Iterate over the dataset and count the occurrences of each class label\nfor i in range(len(dataset)):\n    _, target = dataset[i]\n    labels = target['labels']\n    for label in labels:\n        class_counts[dataset.classes[label]] += 1\n\n# Extract the class labels and counts\nlabels = [str(label) for label in class_counts.keys()]  # Convert labels to strings\ncounts = list(class_counts.values())\n\n# Set up the subplots\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\nbackground_color = '#faf9f4'\nax1.set_facecolor(background_color)\nax2.set_facecolor(background_color)\n\n# Plot the pie chart\ncolors = ['blue', 'green', 'red']\nax1.pie(counts, wedgeprops=dict(width=0.3, edgecolor='w'), labels=labels,\n        colors=colors, radius=1, startangle=120, autopct='%1.2f%%')\nax1.set_title('Class Distribution (Pie Chart)')\n\n# Plot the bar chart\nax2.bar(labels, counts, color='maroon', width=0.4)\nax2.set_xlabel('Class Labels')\nax2.set_ylabel('Counts')\nax2.set_title('Class Distribution (Bar Chart)')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_bbox(img, target, ax):\n    \n    # display the image\n    ax.imshow(img)\n    \n    for (box, label) in zip(target['boxes'], target['labels']):\n        x, y, width, height = box[0], box[1], box[2] - box[0], box[3] - box[1]\n        \n        # create a rectangle patch with different colors i.e. red: without mask, green: without mask, blue: mask weared incorrect\n        if(label == 1):\n            rect = patches.Rectangle((x, y), width, height, linewidth=1, edgecolor='g', facecolor='none')\n            ax.annotate('with mask', (x, y), color='g')\n            \n        elif(label == 2):\n            rect = patches.Rectangle((x, y), width, height, linewidth=1, edgecolor='r', facecolor='none')\n            ax.annotate('without mask', (x, y), color='r')\n            \n        else:\n            rect = patches.Rectangle((x, y), width, height, linewidth=1, edgecolor='b', facecolor='none')\n            ax.annotate('mask weared incorrect', (x, y), color='b')\n        \n        # add the patch to the Axes\n        ax.add_patch(rect)\n        \n        \n    #plt.show()\n# create a single plot with a 1x1 grid\nfig, ax = plt.subplots(1, 1, figsize=(12, 8))\n\n# plotting the image with bounding boxes\nimg, target = dataset[6]\nplot_bbox(img, target, ax)    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_transform(train):\n    \n    if train:\n        return A.Compose(\n        [\n            ToTensorV2(p=1.0)\n        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']}\n        )\n    else:\n        return A.Compose(\n        [\n            ToTensorV2(p=1.0)\n        ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']}\n        )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def collate_func(batch):\n    return tuple(zip(*batch))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the dataset to get the labels for stratification\ndata = MaskDataset(images_dir, annotations_dir, X, y, 450, 350)\n\n# Get the labels from the dataset\nlabels = [data[idx][1]['labels'] for idx in range(len(data))]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split into train & temp\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=labels)\n\n# Split into valid & test\nX_valid, X_test, y_valid, y_test = train_test_split(X_valid, y_valid, test_size=0.3, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create instances of MaskDataset for train, validation, and test sets\ntrain_data = MaskDataset(\n    images_dir,\n    annotations_dir,\n    X_train, \n    y_train,\n    450, 350,\n    T = get_transform(train=True)\n)\n\nvalid_data = MaskDataset(\n     images_dir,\n     annotations_dir,\n     X_valid,\n     y_valid,\n     450, 350,\n     T = get_transform(train=False)\n )\n\ntest_data = MaskDataset(\n     images_dir,\n     annotations_dir,\n     X_test,\n     y_test,\n     450, 350,\n     T = get_transform(train=False)\n )\n\nprint('Length of training set:', len(train_data), '\\nLength of validation set:', len(valid_data), '\\nLength of test set:', len(test_data))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CFG = {\n    'n_epochs': 20,\n    'lr': 0.0001,\n    'batch_size' : 8,\n    'num_workers': 2,\n    'num_classes' : 4,\n    'momentum' : 0.9,\n}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataloader = torch_data.DataLoader(\n    train_data,\n    batch_size = CFG['batch_size'],\n    shuffle = True,\n    num_workers = CFG['num_workers'],\n    collate_fn = collate_func,\n)\n\nvalid_dataloader = torch_data.DataLoader(\n    valid_data,\n    batch_size = CFG['batch_size'],\n    shuffle = False,\n    num_workers = CFG['num_workers'],\n    collate_fn = collate_func,\n)\ntest_dataloader = torch_data.DataLoader(\n    test_data,\n    batch_size = CFG['batch_size'],\n    shuffle = False,\n    num_workers = CFG['num_workers'],\n    collate_fn = collate_func,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nnum_classes = CFG['num_classes']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load fasterrcnn model\ndef fasterrcnn_model(num_classes):\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False)\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n    return model\n\n# Get a model\ndef load_model():\n    model = fasterrcnn_model(num_classes).to(device)\n\n    model = torch.load('/kaggle/input/office-masks/Kaggle_model_weights-2.pth', map_location=device)\n    model.eval()\n    return model\nmodel = load_model()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.SGD(model.parameters(), lr = CFG['lr'], momentum = CFG['momentum'])\nloss_fn = nn.CrossEntropyLoss()\nn_epochs = CFG['n_epochs']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_model(\n    model,\n    optimizer,\n    n_epochs,\n    train_dataloader,\n    valid_dataloader,\n    device,\n):\n    dur = []\n    start_time = time.time()\n    for epoch in range(n_epochs):\n        \n        t0 = time.time()\n        train_loss = 0\n        valid_loss = 0\n        model.train()\n        \n        \n        for images, annotations in (train_dataloader):\n            images = list(image.to(device) for image in images)\n            annotations = [{a: n.to(device) for a, n in t.items()} for t in annotations]\n            \n            optimizer.zero_grad()\n\n            output = model(images, annotations)\n            loss = sum(loss for loss in output.values())\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n\n        train_loss /= len(train_dataloader)\n        \n    \n        # model.eval()\n\n        for images, annotations in (valid_dataloader):\n            images = list(image.to(device) for image in images)\n            annotations = [{a: n.to(device) for a, n in t.items()} for t in annotations]\n            \n            optimizer.zero_grad()\n            \n            with torch.no_grad():\n                \n                output = model(images, annotations)\n            \n            loss = sum(loss for loss in output.values())\n            valid_loss += loss.item()\n        valid_loss /= len(valid_dataloader)\n        \n        evaluate(model, valid_dataloader, device=device)\n        dur.append(time.time() - t0)\n        \n        print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}, Time: {:.4f}'.format(epoch, train_loss, valid_loss, np.mean(dur)))\n\n    torch.save(model, '/kaggle/working/Custom_model_weights.pth')\n    print('Training finished, took {:.2f}s'.format(time.time() - start_time))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_model(model, optimizer, n_epochs, train_dataloader, valid_dataloader, device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prediction_filter(prefinal_pred, threshold):\n    \n    filter_mask = prefinal_pred['scores'] > threshold\n    \n    prefinal_pred['boxes'] = prefinal_pred['boxes'][filter_mask]\n    prefinal_pred['scores'] = prefinal_pred['scores'][filter_mask]\n    prefinal_pred['labels'] = prefinal_pred['labels'][filter_mask]\n    return prefinal_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def apply_nms(prefinal_pred, threshold):\n    # return the indices of the bboxes to keep\n    keep = torchvision.ops.nms(prefinal_pred['boxes'], prefinal_pred['scores'], threshold)\n    #final_pred = prefinal_pred\n    \n    preds_filter = prediction['scores']\n    prefinal_pred['boxes'] = prefinal_pred['boxes'][keep]\n    prefinal_pred['scores'] = prefinal_pred['scores'][keep]\n    prefinal_pred['labels'] = prefinal_pred['labels'][keep]\n    return prefinal_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to convert a torchtensor back to PIL image\ndef torch_to_pil(img):\n    return T.ToPILImage()(img).convert('RGB')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pick one image from the test set\nimg, target = test_data[2]\n# put the model in evaluation mode\nmodel.eval()\nwith torch.no_grad():\n    prediction = model([img.to(device)])[0]\n\nprint('Predicted number of boxes: ', len(prediction['labels']))\nprint('Real number of boxes: ', len(target['labels']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pick one image from the test set\nimg, target = test_data[2]\n\n# put the model in evaluation mode\nmodel.eval()\nwith torch.no_grad():\n    prediction = model([img.to(device)])[0]\n\n# create a 1x4 grid of subplots\nfig, axs = plt.subplots(2, 2, figsize=(22, 22))\n\n# plot each image with bounding boxes in a separate subplot\nprint('Expected Output: ', len(target['labels']))\nplot_bbox(torch_to_pil(img), target, axs[0][0]) \naxs[0][0].set_title('Expected Output', fontsize = 18)\n\nprediction = {x: y.cpu() for x, y in prediction.items()}\nprint('Model Output: ', len(prediction['labels']))\nplot_bbox(torch_to_pil(img), prediction, axs[1][0])\naxs[1][0].set_title('Model Output', fontsize = 18)\n\nfiltered_prediction = prediction_filter(prediction, threshold = 0.5)\nprint('Predicted Filtered Outputs: ', len(filtered_prediction['labels']))\nplot_bbox(torch_to_pil(img), filtered_prediction, axs[0][1])\naxs[0][1].set_title('Filtered Predictions', fontsize = 18)\n\nnms_prediction = apply_nms(filtered_prediction, threshold = 0.5)\nprint('NMS Applied Model Output', len(nms_prediction['labels']))\nplot_bbox(torch_to_pil(img), nms_prediction, axs[1][1])\naxs[1][1].set_title('NMS Prediction', fontsize = 18)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(test_data)):\n    img, target = test_data[i]\n    # Perform further operations with the image and target\n\n    # put the model in evaluation mode\n    model.eval()\n    with torch.no_grad():\n        prediction = model([img.to(device)])[0]\n    \n    # create grid of subplots\n    fig, axs = plt.subplots(2, 2, figsize=(22, 22))\n\n    # plot each image with bounding boxes in a separate subplot\n    \n    plot_bbox(torch_to_pil(img), target, axs[0][0])\n    print('Expected Output: ', len(target['labels']))\n    axs[0][0].set_title('Expected Output', fontsize = 18)\n\n    prediction = {x: y.cpu() for x, y in prediction.items()}\n    print('Model Output: ', len(prediction['labels']))\n    plot_bbox(torch_to_pil(img), prediction, axs[1][0])\n    axs[1][0].set_title('Model Output', fontsize = 18)\n\n    filtered_prediction = prediction_filter(prediction, threshold = 0.5)\n    print('Predicted Filtered Outputs: ', len(filtered_prediction['labels']))\n    plot_bbox(torch_to_pil(img), filtered_prediction, axs[0][1])\n    axs[0][1].set_title('Filtered Predictions', fontsize = 18)\n\n    nms_prediction = apply_nms(filtered_prediction, threshold = 0.5)\n    print('NMS Applied Model Output', len(nms_prediction['labels']))\n    plot_bbox(torch_to_pil(img), nms_prediction, axs[1][1])\n    axs[1][1].set_title('NMS Prediction', fontsize = 18)\n\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}